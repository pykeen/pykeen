metadata:
  title: Learn CoDEx-Large Dataset with NodePiece + RotatE (no relational tokenization) as described by Galkin et al., 2022
pipeline:
  random_seed: 42
  dataset: codexlarge
  dataset_kwargs:
    create_inverse_triples: True
  evaluator_kwargs:
    filtered: true
  loss: BCEWithLogitsLoss
  loss_kwargs:
    reduction: 'mean'
  model: NodePiece
  model_kwargs:
    entity_initializer: xavier_uniform_
    relation_initializer: init_phases
    relation_constrainer: complex_normalize
    embedding_dim: 200   # for both anchors and relations
    interaction: RotatEInteraction
    aggregation: mlp
    tokenizers: AnchorTokenizer
    num_tokens: 20  # 20 anchors
    tokenizers_kwargs:
      - selection: MixtureAnchorSelection
        searcher: CSGraphAnchorSearcher
        selection_kwargs:
          selections: ['Degree', 'PageRank', 'Random']
          ratios: [0.4, 0.4, 0.2]
          num_anchors: 7000

  optimizer: Adam
  optimizer_kwargs:
    lr: 0.0005
  training_kwargs:
    batch_size: 256
    num_epochs: 121
    label_smoothing: 0.3
  training_loop: LCWA
results:  # https://openreview.net/pdf?id=xMJWUKJnFSw Table 3 row 4
  best:
    hits_at_k:
      '10': 0.332
    mean_reciprocal_rank: 0.201
