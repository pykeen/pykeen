# Official code: https://github.com/MichSchli/RelationPrediction
# Arxiv: https://arxiv.org/abs/1703.06103, last update: Thu, 26 Oct 2017
# Conference Proceddings: https://link.springer.com/chapter/10.1007/978-3-319-93417-4_38
metadata:
  title: Learn FB15k Dataset with R-GCN as described by Schlichtkrull et al., 2018
pipeline:
  dataset: fb15k
  evaluator_kwargs:
    filtered: true
  # > We optimize for cross-entropy loss 
  loss: BCEWithLogits
  loss_kwargs:
    reduction: mean
  model: R-GCN
  # >  For FB15k and WN18, we report results using basis decomposition (Eq. 3) with two basis
  # > functions, and a single encoding layer with 200-dimensional embeddings.
  # > We regularize the encoder through edge dropout applied before normalization, with dropout
  # > rate 0.2 for self-loops and 0.4 for other edges. 
  # > We apply l2 regularization to the decoder with a penalty of 0.01.
  model_kwargs:
    base_entity_initializer: xavier_uniform
    decomposition: bases
    decomposition_kwargs:
      num_bases: 2
    edge_dropout: 0.4
    edge_weighting: InverseInDegree
    embedding_dim: 200
    interaction: distmult
    num_layers: 1
    relation_initializer: xavier_uniform
    self_loop_dropout: 0.2
    regularizer: Lp
    regularizer_kwargs:
      apply_only_once: false
      dim: null
      normalize: false
      p: 2.0
      weight: 0.01
  # > we maintain the same number of negative samples (i.e. Ï‰ = 1).
  # https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/settings/gcn_basis.exp#L37
  # => configuration in repository has num_negs_per_pos = 10
  negative_sampler: basic
  negative_sampler_kwargs:
    num_negs_per_pos: 1
  # > We use the Adam optimizer (Kingma and Ba 2014) with a learning rate of 0.01.
  # https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/settings/gcn_basis.exp#L32
  optimizer: Adam
  optimizer_kwargs:
    lr: 0.01
  training_kwargs:
    # > We use full-batch optimization for both the baselines and our model
    # https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/settings/gcn_basis.exp#L40
    # => code has a batch-size
    batch_size: 30000
    # https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/settings/gcn_basis.exp#L25
    gradient_clipping_max_norm: 1.0
    num_epochs: 500
    sampler: schlichtkrull
  training_loop: SLCWA
results:
  # results are computed using pessimistic aka worst rank definition, cf.
  # https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/evaluation.py#L151-L152
  worst:
    hits_at_k:
      '1': 0.541
      '10': 0.825
      '3': 0.736
    mean_reciprocal_rank: 0.651
